{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountainproject.com Rock Climbing Route Recommender System\n",
    "## Mike Bell\n",
    "### http://github.io/mikebell7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection \n",
    "\n",
    "This notebook contains the code used to scrape area/route data from mountainproject.com (MP).\n",
    "\n",
    "We use the requests and BeautifulSoup packages to recursively scrape MP's route/area directory tree\n",
    "structure. \n",
    "\n",
    "Some things to note:\n",
    "\n",
    "\n",
    "1. The mountainproject directory is organized in a tree consisting of 'area' pages, all of which have a sidebar of links to the smaller subareas contained in that area, or links to the climbing routes found in that area. It is important to note that area pages which contain climbing routes do NOT contain links to any other subareas, so such areas are considered leaf nodes in the area directory tree. Hence, when recursively scraping the directory tree we can easily identify leaves by the presence of any route pages \n",
    "2. Area Pages have URL: www.mountainproject.com/area/AREA_ID/AREA_NAME\n",
    "3. Route Pages have URL: www.mountainproject.com/route/ROUTE_ID/ROUTE_NAME\n",
    "4. Areas and routes are identified by unique 9-digit IDs, appearing as AREA_ID or ROUTE_ID in their repsective urls shown above.\n",
    "5. Registered mountainproject.com users have a unique userID, which is a string of integers of variable length.\n",
    "5. Mountainproject.com users are able to 'rate' climbing routes on a scale of 0 - 4 stars. \n",
    "6. For each route, we collect its average rating along with a list of individual userIDs and ratings. \n",
    "\n",
    "- The 'root' nodes for what I call 'area trees' on mountainproject.com are: Each of the 50 states 'Alabama', ... 'Wyoming', 'International', and '* In Progress'. For this project we ignore the 'International' and '* In Progress ' pages, and recursively scrape all areas and routes in all 50 US states.\n",
    "\n",
    "\n",
    "- The recursive scraping algorithm is as follows:\n",
    "For the current area page, we collect all relevant area information (areaID, area name, GPS coordinates, elevation, description etc.) as well as all sidebar links. If the sidebar links contain /route/ URLs, then this area page is a leaf node and we iterate through the list of routes on this page and scrape and store all relevant information from these pages (routeID, route name, description, average star rating, list of users and ratings). \n",
    "\n",
    "\n",
    "- Else, if the current sidebar links are to area pages (subareas), we add each of these subareas URLs to a queue and recursively scrape each of these subareas as above. \n",
    "\n",
    "- To scrape most efficiently, we first traverse the area/route tree of a state and collect a list of URLs for each area and route, tracking parent information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the 9-digit area/route id from a given url\n",
    "def get_id(url):\n",
    "    return int(url.split('/')[4])\n",
    "\n",
    "# checks if a given url is of 'route' type, all route pages are of the form \n",
    "# www.mountainproject.com/route/ROUTE_ID/ROUTE_NAME\n",
    "def is_route(url):\n",
    "    return url.split('/')[3] == 'route'\n",
    "\n",
    "# Extract all relevant information from an area page\n",
    "def read_area(url):\n",
    "    try:         \n",
    "        res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        requests.session().close()\n",
    "\n",
    "        # An area will initially be a dictionary with the following keys\n",
    "        # name, id, parent_id, elevation, latitude, longitude\n",
    "        area = {}\n",
    "        \n",
    "        # get area name and id \n",
    "        area['name'] =  soup.find('h1').contents[0].strip() \n",
    "        area['id'] = get_id(url)\n",
    "\n",
    "        # get the parent area ID, if it has one\n",
    "        if len(soup.find('div', {'class' : 'mb-half small text-warm'}).find_all('a')) > 1:\n",
    "            area['parent_id'] = get_id(soup.find('div', {'class' : 'mb-half small text-warm'}).find_all('a')[-1]['href'])\n",
    "        else:\n",
    "            area['parent_id'] = None\n",
    "\n",
    "        # Elevation and GPS coordinates (if present) are found in the 'description-details' section of an area\n",
    "        description_details = soup.find('table', {'class' : 'description-details'}).find_all('td')\n",
    "\n",
    "        # get elevation, lat/lon coordinates, if present (not all areas have this information)\n",
    "        area['elevation'] = None\n",
    "        area['latitude'] = None\n",
    "        area['longitude'] = None    \n",
    "\n",
    "        for i, desc in enumerate(description_details):\n",
    "            if 'Elevation' in desc.text:\n",
    "                area['elevation'] = description_details[i+1].text.strip().split('ft')[0].strip()\n",
    "            if 'GPS' in desc.text:\n",
    "                gps = description_details[i+1].contents[0].strip().split(',')\n",
    "\n",
    "                area['latitude'] = gps[0]\n",
    "                area['longitude'] = gps[1]\n",
    "        return area\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        return {}\n",
    "    \n",
    "# Extract all relevant information from a route page    \n",
    "def read_route(url):   \n",
    "    try:\n",
    "        res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        requests.session().close()\n",
    "        description_details = soup.find('table', {'class' : 'description-details'}).find_all('td')\n",
    "\n",
    "        route = {}\n",
    "        # A route will initially be a dictionary with the following keys\n",
    "        # name, id, area_id, description, type, height, pitches, grade, score, number of votes\n",
    "        # star_ratings: a dictionary of pairs consisting of user_id :rating for each user rating \n",
    "        # for the given route\n",
    "        \n",
    "        # get route description\n",
    "        descr = [x.strip() for x in description_details[1].text.split(',')]\n",
    "\n",
    "        route['name'] = soup.find('h1').text.strip()    \n",
    "        route['id'] = get_id(url)\n",
    "        \n",
    "        # We are only interested in routes of type trad/sport/boulder, ignore all others\n",
    "        if any([ignore_type in descr for ignore_type in ['Ice', 'Aid', 'Mixed', 'Alpine']]):\n",
    "            return {}\n",
    "        if 'Trad' in descr:\n",
    "            route['type'] = 'Trad'\n",
    "        elif 'Sport' in descr:\n",
    "            route['type'] = 'Sport'\n",
    "        elif 'Boulder' in descr:\n",
    "            route['type'] = 'Boulder'\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "        # Get the parent areaID\n",
    "        route['area_id'] = get_id(soup.find('div', {'class' : 'mb-half small text-warm'}).find_all('a')[-1]['href'])\n",
    "\n",
    "        # get number of pitches and height, grade, score, and votes\n",
    "        route_pitches = 1\n",
    "        route_height = 0\n",
    "\n",
    "        for token in descr: \n",
    "            if 'ft' in token: \n",
    "                route_height = int(token.split('ft')[0])\n",
    "            if 'pitches' in token: \n",
    "                route_pitches = int(token.split('pitches')[0])\n",
    "\n",
    "        route['grade'] =  soup.find('span', {'class' :\"rateYDS\"}).text.split()[0]    \n",
    "        route['height'] = route_height\n",
    "        route['pitches'] = route_pitches\n",
    "\n",
    "        route_score_str = soup.find('span', {'id' : f'starsWithAvgText-{route[\"id\"]}'}).text.split()\n",
    "        route['score'] = float(route_score_str[1])\n",
    "        route['votes'] = int(route_score_str[3].replace(',',''))\n",
    "        route['description'] = soup.find('div', {'class' : 'fr-view'}).text  \n",
    "\n",
    "        # get the link to the page containing the users and ratings\n",
    "        stats_url = soup.find('span', {'id' : 'route-star-avg'}).find('a')['href']\n",
    "\n",
    "        # If a route has any user ratings, scrape the associated ratings page for\n",
    "        # all users and their ratings\n",
    "        if route['votes'] > 0:\n",
    "            route['star_ratings'] = get_star_ratings(stats_url)\n",
    "        else:\n",
    "            route['votes'] = None\n",
    "\n",
    "        return route\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        return {}\n",
    "    \n",
    "# Extract the star ratings for a given route, tracking the unique userID and corresponding rating\n",
    "def get_star_ratings(url):\n",
    "    try:\n",
    "        res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        requests.session().close()\n",
    "\n",
    "        stats = soup.find('table',{'class':'table table-striped'})\n",
    "        \n",
    "        # Route ratings are displayed as a table row element with a link to the user's page\n",
    "        # followed by a number of star images, corresponding to the given rating\n",
    "        #\n",
    "        # We extract the userID from the page link and count the number of star images associated to this user\n",
    "        #\n",
    "        # What results is a dictionary with entries of the form {userID: star_rating}.\n",
    "        star_ratings = {}\n",
    "        \n",
    "        for user in stats.find_all('tr'):\n",
    "            star_ratings[get_id(user.find('a')['href'])] = len(user.find_all('img', {'src' : \"/img/stars/starBlue.svg\"}))            \n",
    "        return star_ratings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        return {}\n",
    "\n",
    "\n",
    "# This is the main recursive function which traverses the area tree\n",
    "# The parameters n and start can be used to in case of disconnection during scraping\n",
    "# to start from the last point the backup was saved.\n",
    "#\n",
    "# Note that this function does not yet navigate the route pages\n",
    "# it simply logs the area-tree structure encountered, along with the URLs of any routes contained in \n",
    "# node area pages. \n",
    "#\n",
    "# Once an area tree has been collected completely, we then run through the list of routes and \n",
    "# scrape the route info / ratings.\n",
    "def get_areas_route_list(url, areas, route_list, name, n = 0, start = 0):\n",
    "    \n",
    "    # If the current URL is a route, extract the route information and add to the route list.\n",
    "    # We keep a backup \n",
    "    if is_route(url):\n",
    "        route_list.append(url)\n",
    "        if len(route_list) % 500 == 0:\n",
    "            pd.DataFrame(areas).to_csv(f'./data/{name}_areas_backup.csv', index = False)\n",
    "            pd.DataFrame(route_list).to_csv(f'./data/{name}_route_list_backup.csv', index = False)\n",
    "    else: \n",
    "        \n",
    "        print('-'*n + f'AREA: {url}')\n",
    "        areas.append(read_area(url))\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            requests.session().close()\n",
    "            \n",
    "            \n",
    "            sidebar = soup.find('div', {'class' : 'mp-sidebar'})\n",
    "            \n",
    "            # Area which contain further sub-areas will contain a div of class 'lef-nav-row'\n",
    "            # in its sidebar\n",
    "            # Gather all sub-area URLs \n",
    "            urls = sidebar.find_all('div', {'class':'lef-nav-row'})\n",
    "            \n",
    "            # If no 'lef-nav-row' was found, we have reached a node-area\n",
    "            # and now we look for the route table and collect all listed route URLs\n",
    "            if urls == []: \n",
    "                sidebar = soup.find('div', {'class' : 'mp-sidebar'})\n",
    "                if(sidebar.find('table', {'id':'left-nav-route-table'})):\n",
    "                    urls = sidebar.find('table', {'id':'left-nav-route-table'}).find_all('tr')\n",
    "\n",
    "            # Now recursively scrape all URLs found in the above, either the sub-areas or route pages\n",
    "            for i,row in enumerate(urls):\n",
    "                #time.sleep(2)\n",
    "                if n == 0:\n",
    "                    if i >= start:\n",
    "                        if row.find('a'):\n",
    "                            sub_url = row.find('a')['href']\n",
    "                            get_areas_route_list(sub_url, areas, route_list,name, n+1)\n",
    "                else:\n",
    "                    if row.find('a'):\n",
    "                        sub_url = row.find('a')['href']\n",
    "                        get_areas_route_list(sub_url, areas, route_list,name, n+1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(url)                           \n",
    "\n",
    "# This function runs through the list of routes collected by the above function\n",
    "# and reads the route info/ratings. \n",
    "def read_route_list(name, routes, route_list, count, start = 0):       \n",
    "    for url in route_list[start:]:\n",
    "        if is_route(url):\n",
    "            route = read_route(url)\n",
    "            if route != {}:\n",
    "                routes.append(read_route(url))\n",
    "  \n",
    "            if count % 100 == 0:\n",
    "                \n",
    "                print(f'count: {count}, routes: {len(routes)-1}')\n",
    "                pd.DataFrame(routes).to_csv(f'./data/{name}_routes_{start}.csv', index = False)\n",
    "        count += 1\n",
    "        \n",
    "    pd.DataFrame(routes).to_csv(f'./data/{name}_routes_{start}.csv', index = False)          \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of climbing grades for roped routes and boulders, in order according to difficulty.\n",
    "# Sport/Trad climbing routes and Bouldering routes are graded according to different diffuclty scales:\n",
    "# The Yosemite Decimal System (YDS) and V-Scale, respectively.\n",
    "# \n",
    "# See https://www.mountainproject.com/international-climbing-grades for a complete list.\n",
    "def get_grades():\n",
    "    grades_url = 'https://www.mountainproject.com/international-climbing-grades'\n",
    "    res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    requests.session().close()\n",
    "\n",
    "    tables = soup.find_all('table', {'class':'table table-condensed table-striped'})\n",
    "\n",
    "    rows = tables[0].find_all('tr')\n",
    "    rank = 0\n",
    "    climb_grades = {}\n",
    "    for row in rows:\n",
    "        if(row.find('td')):\n",
    "            grade = row.find('td').text\n",
    "            climb_grades[grade] = rank\n",
    "            rank += 1\n",
    "\n",
    "    rows = tables[1].find_all('tr')\n",
    "\n",
    "\n",
    "    boulder_grades = {}\n",
    "    rank = 0\n",
    "    for row in rows:\n",
    "        if(row.find('td')):\n",
    "            grade = row.find('td').text\n",
    "            boulder_grades[grade] = rank\n",
    "            rank += 1\n",
    "\n",
    "    climb_grades = pd.DataFrame.from_dict(climb_grades, orient = 'index',columns=['rank']).reset_index().rename(columns = {'index' : 'grade'})\n",
    "    boulder_grades = pd.DataFrame.from_dict(boulder_grades, orient = 'index',columns=['rank']).reset_index().rename(columns = {'index' : 'grade'})\n",
    "\n",
    "    climb_grades.to_csv('./data/climb_grades.csv', index = False)\n",
    "    boulder_grades.to_csv('./data/boulder_grades.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted state scraper by number of routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state and state area URL data\n",
    "# This gives a list of the 'root' nodes for our area-tree traversal process\n",
    "# We order the list of states by the number of routes contained in each state\n",
    "# In this way, we can organize our scraping from smallest to largest\n",
    "#\n",
    "url = 'http://www.mountainproject.com/'\n",
    "res = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "requests.session().close()\n",
    "\n",
    "states = []\n",
    "ignore = ['International', '* In Progress']\n",
    "for col in soup.find('div', {'id' : 'route-guide'}).find_all('div' , {'class' : 'col-sm-3 hidden-md-down'}):\n",
    "    for row in col.find_all('strong'):\n",
    "        state = {}\n",
    "        state['state'] = row.find('a').text\n",
    "        state['url'] = row.find('a').attrs['href']\n",
    "        state['size'] = int(row.find('small').text.replace(',',''))\n",
    "        if(state['state'] not in ignore):\n",
    "            states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_states = sorted(list(map( lambda x : (x['state'], x['url'], x['size']), states)),key = lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following can be used to ignore certain states, and start other states at different \n",
    "# levels of the recursion tree, in case of interrupted scraping / loading backups.\n",
    "completed = []\n",
    "routes = {}\n",
    "starts = [0]*len(sorted_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all areas and route lists from states from smallest to largest \n",
    "import os\n",
    "\n",
    "for i,state in enumerate(sorted_states):\n",
    "    if state[0] not in completed:\n",
    "        areas = []\n",
    "        route_list = []\n",
    "        print('Starting ',state[0])\n",
    "        \n",
    "        # See if the route/area lists already exist for the current state. If it does, read it in from\n",
    "        # the existing files, otherwise begin recursively scraping the area tree for that state\n",
    "        # and gather area/route URL info.\n",
    "        if os.path.exists(f'./data/{state[0]}_route_list.csv') and os.path.exists(f'./data/{state[0]}_areas.csv'):\n",
    "            route_list = pd.read_csv(f'./data/{state[0]}_route_list.csv') \n",
    "            areas = pd.read_csv(f'./data/{state[0]}_areas.csv')\n",
    "            print(f'{state[0]} RL and Areas found')\n",
    "        else:        \n",
    "            print(\"Route list not find, generating.\")\n",
    "            get_areas_route_list(state[1], areas, route_list, state[0])                 \n",
    "            pd.DataFrame(areas).to_csv(f'./data/{state[0]}_areas.csv', index = False)\n",
    "            pd.DataFrame(route_list).to_csv(f'./data/{state[0]}_route_list.csv', index = False) \n",
    "        \n",
    "        # Now read in the route list for the current state (which either exists from above, or has been generated)\n",
    "        rl = list(pd.read_csv(f'./data/{state[0]}_route_list.csv').values.flatten())\n",
    "        print(f\"Reading {state[0]}'s {state[2]} routes.\")\n",
    "        \n",
    "        # For each route, scrape the route info and uer ratings.\n",
    "        routes[state[0]] = []\n",
    "        read_route_list(state[0], routes[state[0]] , rl, starts[i])\n",
    "        pd.DataFrame(routes[state[0]]).to_csv(f\"./data/{state[0]}_routes.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
